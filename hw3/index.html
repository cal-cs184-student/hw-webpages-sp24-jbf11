<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
</style>
<title>CS 284A Raytracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>

<body>

<h1 align="middle"><a href="https://cal-cs184-student.github.io/hw-webpages-sp24-jbf11/hw2/index.html">Webpage</a></h1>
<h1 align="middle">CS 284A: Computer Graphics and Imaging, Spring 2024</h1>
<h1 align="middle">Project 3: Raytracer</h1>
<h2 align="middle">Dawson Do and Joshua Fernandes</h2>

<br><br>

<div>

<h2 align="middle">Overview</h2>
<p>
    Overall, in this assignment, we were able to develop a raytracer and implement a global illumination algorithm. First, in Part 1, we generated rays and wrote methods to intersect our rays with scene objects. This allowed us to render objects by coloring them according to their surface normals and gave us experience with manipulating camera and world coordinates. In Part 2, we got experience with acceleration structures, implementing a bounding volume hierarchy, in which we sorted the scene objects into a tree such that we could efficiently ray trace. Rather than testing each ray against each object, we could test each ray against the tree, reducing computation of intersection to logarithmic rather than linear time per ray (in the number of scene objects). This is necessary for rendering complex scenes.
</p>
<p>
    In Part 3, we handled direct illumination, in which we were able to test Monte Carlo integration methods of lighting. We wrote two methods, one that sampled over the hemisphere and one that sampled over the light source. With this exercise, we learned the importance of sampling the regions where you expect contributions, as lighting sampling proved much more robust than hemisphere lighting, particularly for small light sources. In Part 4, we handled indirect illumination, which is simply computed by sampling reflection of light and recursively computing direct illumination. Through this part, we understood how to perform a global light transport simulation and capture the effects of indirect lighting. We also implemented a Russian roulette algorithm. Finally, in Part 5, we sampled adaptively. This demonstrated the dramatic speedup (order of magnitude) that is possible by on-the-fly estimation of errors and terminating locally when error is small. Essentially, we used resources where they were most needed rather than treating the entire domain as equivalent. This assignment gave us a holistic understanding of ray tracing and illumination algorithms, including the relatively large amount of resources needed for rendering seemingly simple scenes.
</p>
<br>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection</h2>

<br>

<h3 align="left">Walk through the ray generation and primitive intersection parts of the rendering pipeline.</h3>

<p>
    For each pixel, we generate $\texttt{ns_aa}$ number of rays to sample the scene. First, we use $\texttt{gridSampler->get_sample()}$ to get a random sample from $[0,1]\times[0,1]$ and add that to origin, the bottom left corner of the pixel. This point is in the image space. We then call $\texttt{camera->generate_ray()}$ to return a ray that goes from the camera in camera space to that point after it has been transformed from image space onto the sensor plane in camera space. After normalization, this becomes the direction for the ray (camera to sensor coordinate), while the origin is set to be the camera position. 
</p>

<p>
    We trace the trajectory of this ray into the world using $\texttt{est_radiance_global_illumination()}$ which returns the radiance at the point in the scene this ray intersects. We determine if the ray intersects with a primitive using $\texttt{bvh->intersect()}$. Within this function, we iterate through all the primitives, checking if the ray hits any and updating the intersection point if we find that the ray hits a primitive that is earlier in its trajectory than the previously-recorded closest intersection. 
</p>

<p>
    We trace the trajectory of this ray into the world using $\texttt{est_radiance_global_illumination()}$ which returns the radiance at the point in the scene this ray intersects. We determine if the ray intersects with a primitive using $\texttt{bvh->intersect()}$. Within this function, we iterate through all the primitives, checking if the ray hits any and updating the intersection point if we find that the ray hits a primitive that is earlier in its trajectory than the previously-recorded closest intersection. 
</p>
<p>
  After the closest intersection is found, $\texttt{est_radiance_global_illumination()}$ returns the radiance at this point. The radiance returned by each of the $\texttt{ns_aa}$ sample rays are totaled, then averaged, and we finally update the sample buffer with this value. 
</p>

<br>

<h3 align="left">Explain the triangle intersection algorithm you implemented in your own words.</h3>

<p>
  To implement triangle intersection, we first compute the plane intersection of the ray with the plane on which the triangle lies. We computed this plane by computing the triangle normal vector from the cross product of two of the vectors defined by the triangle’s edges. We first check if the ray is parallel to the plane (within a small tolerance) and exit if true. If the corresponding intersection point in the parameter t was outside of the range defined by the minimum and maximum, we exited with false. If it was within the permissible range, we then computed if the intersection point was within the triangle. To do so, we computed the barycentric coordinates of the point with respect to the triangle and checked if each of the coordinates was positive. We returned true only if we determined the intersection point with the plane to lie within the triangle. We computed the normal at the point by the barycentric average of the normals of each vertex.
</p>

<br>

<h3 align="left">Show images with normal shading for a few small .dae files.</h3>

<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/part1/cube.png" align="middle" width="400px"/>
        <figcaption>cube.dae</figcaption>
      </td>
      <td>
        <img src="images/part1/CBempty.png" align="middle" width="400px"/>
        <figcaption>CBempty.dae</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/part1/CBspheres.png" align="middle" width="400px"/>
        <figcaption>CBspheres.dae</figcaption>
      </td>
      <td>
        <img src="images/part1/banana.png" align="middle" width="400px"/>
        <figcaption>banana.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h2 align="middle">Part 2: Bounding Volume Hierarchy</h2>

<br>

<h3 align="left">Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.</h3>

<p>
    We construct the BVH using a recursive algorithm. Given the (sub) vector of primitives, we first create the bounding box that holds all the primitives. If the number of primitives is less than or equal to the max leaf size, we create a leaf node by assigning its start and end iterators to those passed in by the function call. Otherwise, we need to determine how to split the bounding box and partition the primitives using $\textt{findBestSplit()}$. 
</p>

<p>
  Our method to find the best split tests $\textt{BINS}-1$ number of uniformly spaced splits across the width of the bounding box, for each of the three axes. In practice, we found that $\textt{BINS}=8$, a total of 27 different split possibilities, was sufficient for speedup. For each axis, we first find the bounding width of all the centroids of the primitives. Then, we determine which of the $\textt{BINS}$ bins a primitive belongs to. Additionally, we compute the bounding box of all the primitives in each bin.
</p>

<p>
  To evaluate the cost of each split we use the surface area heuristic (SAH). For each split, we count and determine the surface area of the bounding box of all primitives on each side. This is achieved in one pass through the bins since we collected the number of primitives in and the bounding box of each bin—we keep a running sum of the left side of the split and the right side as we iterate through each split. Then for each split, we calculate the cost of the split using the SAH and keep track of the minimum, as well as the corresponding axis and split position. Finally, we return the least cost split position, the splitting axis, and the number of primitives on the left of the split ($\textt{l_count}$).
</p>

<p>
  We then must split our vector of primitives that was passed in. First we initialize the non-leaf node with the bounding box of all the primitives. We create the children using recursive calls to $\textt{construct_bvh()}$. Because the function takes in the $\textt{start}$ and $\textt{end}$ iterator of a subvector, we accomplish this split by re-sorting the subvector such that the primitives with a centroid to the left of the split are on the left of the subvector. Then, to call the function recursively to create the left child, we simply pass in start and $\textt{start} + \textt{l_count}$ for the start and end. We pass in $\textt{start} + \textt{l_count}$ and end for the right child.
</p>

<br>

<h3 align="left">Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.</h3>

<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/part2/maxplanck.png" align="middle" width="400px"/>
        <figcaption>maxplanck.dae</figcaption>
      </td>
      <td>
        <img src="images/part2/CBLucy.png" align="middle" width="400px"/>
        <figcaption>CBLucy.dae</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/part2/beast.png" align="middle" width="400px"/>
        <figcaption>beast.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3 align="left">Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.</h3>

<div align="middle">
  <table style="width:100%">
    <tr align="left">
      <td></td>
      <td>teapot.dae</td>
      <td>cow.dae</td>
      <td>beetle.dae</td>
      <td>peter.dae</td>
      <td>beast.dae</td>
    </tr>
    <br>
    <tr align="left">
      <td>Number of Primitives</td>
      <td>2464</td>
      <td>5856</td>
      <td>7558</td>
      <td>40018</td>
      <td>64618</td>
    </tr>
    <br>
    <tr align="left">
      <td>Rendering w/o BVH</td>
      <td>9.1695</td>
      <td>23.8812</td>
      <td>31.2788</td>
      <td>N/A</td>
      <td>N/A</td>
    </tr>
    <br>
    <tr align="left">
      <td>Rendering w/o BVH</td>
      <td>9.1695</td>
      <td>23.8812</td>
      <td>31.2788</td>
      <td>N/A</td>
      <td>N/A</td>
    </tr>
    <br>
    <tr align="left">
      <td>Building BVH</td>
      <td>0.0050</td>
      <td>0.0215</td>
      <td>0.0182</td>
      <td>0.1348</td>
      <td>0.2451</td>
    </tr>
    <br>
    <tr align="left">
      <td>Rendering w/ BVH</td>
      <td>0.0631</td>
      <td>0.0678</td>
      <td>0.0662</td>
      <td>0.0628</td>
      <td>0.0585 (0.1014)</td>
    </tr>
  </table>
</div>
  
</body>
</html>
