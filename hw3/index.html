<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
</style>
<title>CS 284A Raytracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>

<body>

<h1 align="middle"><a href="https://cal-cs184-student.github.io/hw-webpages-sp24-jbf11/hw3/index.html">Webpage</a></h1>
<h1 align="middle">CS 284A: Computer Graphics and Imaging, Spring 2024</h1>
<h1 align="middle">Project 3: Raytracer</h1>
<h2 align="middle">Dawson Do and Joshua Fernandes</h2>

<br><br>

<div>

<h2 align="middle">Overview</h2>
<p>
    Overall, in this assignment, we were able to develop a raytracer and implement a global illumination algorithm. First, in Part 1, we generated rays and wrote methods to intersect our rays with scene objects. This allowed us to render objects by coloring them according to their surface normals and gave us experience with manipulating camera and world coordinates. In Part 2, we got experience with acceleration structures, implementing a bounding volume hierarchy, in which we sorted the scene objects into a tree such that we could efficiently ray trace. Rather than testing each ray against each object, we could test each ray against the tree, reducing computation of intersection to logarithmic rather than linear time per ray (in the number of scene objects). This is necessary for rendering complex scenes.
</p>
<p>
    In Part 3, we handled direct illumination, in which we were able to test Monte Carlo integration methods of lighting. We wrote two methods, one that sampled over the hemisphere and one that sampled over the light source. With this exercise, we learned the importance of sampling the regions where you expect contributions, as lighting sampling proved much more robust than hemisphere lighting, particularly for small light sources. In Part 4, we handled indirect illumination, which is simply computed by sampling reflection of light and recursively computing direct illumination. Through this part, we understood how to perform a global light transport simulation and capture the effects of indirect lighting. We also implemented a Russian roulette algorithm. Finally, in Part 5, we sampled adaptively. This demonstrated the dramatic speedup (order of magnitude) that is possible by on-the-fly estimation of errors and terminating locally when error is small. Essentially, we used resources where they were most needed rather than treating the entire domain as equivalent. This assignment gave us a holistic understanding of ray tracing and illumination algorithms, including the relatively large amount of resources needed for rendering seemingly simple scenes.
</p>
<br>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection</h2>

<br>

<h3 align="left">Walk through the ray generation and primitive intersection parts of the rendering pipeline.</h3>

<p>
    For each pixel, we generate $\texttt{ns_aa}$ number of rays to sample the scene. First, we use $\texttt{gridSampler->get_sample()}$ to get a random sample from $[0,1]\times[0,1]$ and add that to origin, the bottom left corner of the pixel. This point is in the image space. We then call $\texttt{camera->generate_ray()}$ to return a ray that goes from the camera in camera space to that point after it has been transformed from image space onto the sensor plane in camera space. After normalization, this becomes the direction for the ray (camera to sensor coordinate), while the origin is set to be the camera position. 
</p>

<p>
    We trace the trajectory of this ray into the world using $\texttt{est_radiance_global_illumination()}$ which returns the radiance at the point in the scene this ray intersects. We determine if the ray intersects with a primitive using $\texttt{bvh->intersect()}$. Within this function, we iterate through all the primitives, checking if the ray hits any and updating the intersection point if we find that the ray hits a primitive that is earlier in its trajectory than the previously-recorded closest intersection. 
</p>

<p>
    We trace the trajectory of this ray into the world using $\texttt{est_radiance_global_illumination()}$ which returns the radiance at the point in the scene this ray intersects. We determine if the ray intersects with a primitive using $\texttt{bvh->intersect()}$. Within this function, we iterate through all the primitives, checking if the ray hits any and updating the intersection point if we find that the ray hits a primitive that is earlier in its trajectory than the previously-recorded closest intersection. 
</p>
<p>
  After the closest intersection is found, $\texttt{est_radiance_global_illumination()}$ returns the radiance at this point. The radiance returned by each of the $\texttt{ns_aa}$ sample rays are totaled, then averaged, and we finally update the sample buffer with this value. 
</p>

<br>

<h3 align="left">Explain the triangle intersection algorithm you implemented in your own words.</h3>

<p>
  To implement triangle intersection, we first compute the plane intersection of the ray with the plane on which the triangle lies. We computed this plane by computing the triangle normal vector from the cross product of two of the vectors defined by the triangle’s edges. We first check if the ray is parallel to the plane (within a small tolerance) and exit if true. If the corresponding intersection point in the parameter $t$ was outside of the range defined by the minimum and maximum, we exited with false. If it was within the permissible range, we then computed if the intersection point was within the triangle. To do so, we computed the barycentric coordinates of the point with respect to the triangle and checked if each of the coordinates was positive. We returned true only if we determined the intersection point with the plane to lie within the triangle. We computed the normal at the point by the barycentric average of the normals of each vertex.
</p>

<br>

<h3 align="left">Explain the sphere intersection algorithm you implemented in your own words.</h3>

<p>
  To implement sphere intersection, we expand out the equation for intersection of a ray and a sphere, which is quadratic in $t$. We then compute the discriminant and if it is negative, both solutions are imaginary and there is no intersection, so we return. If it is positive, we compute the two intersection points. If neither lies within the minimum to maximum range, we return false. If exactly one lies in the range, we return that point, and if both lie in the range, we return the smaller value (since this intersection is first along the ray). We compute the normal as the unit vector in the direction between the center of the sphere and the intersection point.
</p>

<br>

<h3 align="left">Show images with normal shading for a few small .dae files.</h3>

<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/part1/cube.png" align="middle" width="400px"/>
        <figcaption>cube.dae</figcaption>
      </td>
      <td>
        <img src="images/part1/CBempty.png" align="middle" width="400px"/>
        <figcaption>CBempty.dae</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/part1/CBspheres.png" align="middle" width="400px"/>
        <figcaption>CBspheres.dae</figcaption>
      </td>
      <td>
        <img src="images/part1/banana.png" align="middle" width="400px"/>
        <figcaption>banana.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h2 align="middle">Part 2: Bounding Volume Hierarchy</h2>

<br>

<h3 align="left">Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.</h3>

<p>
    We construct the BVH using a recursive algorithm. Given the (sub) vector of primitives, we first create the bounding box that holds all the primitives. If the number of primitives is less than or equal to the max leaf size, we create a leaf node by assigning its start and end iterators to those passed in by the function call. Otherwise, we need to determine how to split the bounding box and partition the primitives using $\texttt{findBestSplit()}$. 
</p>

<p>
  Our method to find the best split tests $\texttt{BINS}-1$ number of uniformly spaced splits across the width of the bounding box, for each of the three axes. In practice, we found that $\texttt{BINS}=8$, a total of 27 different split possibilities, was sufficient for speedup. For each axis, we first find the bounding width of all the centroids of the primitives. Then, we determine which of the $\texttt{BINS}$ bins a primitive belongs to. Additionally, we compute the bounding box of all the primitives in each bin.
</p>

<p>
  To evaluate the cost of each split we use the surface area heuristic (SAH). For each split, we count and determine the surface area of the bounding box of all primitives on each side. This is achieved in one pass through the bins since we collected the number of primitives in and the bounding box of each bin—we keep a running sum of the left side of the split and the right side as we iterate through each split. Then for each split, we calculate the cost of the split using the SAH and keep track of the minimum, as well as the corresponding axis and split position. Finally, we return the least cost split position, the splitting axis, and the number of primitives on the left of the split ($\texttt{l_count}$).
</p>

<p>
  We then must split our vector of primitives that was passed in. First we initialize the non-leaf node with the bounding box of all the primitives. We create the children using recursive calls to $\texttt{construct_bvh()}$. Because the function takes in the $\texttt{start}$ and $\texttt{end}$ iterator of a subvector, we accomplish this split by re-sorting the subvector such that the primitives with a centroid to the left of the split are on the left of the subvector. Then, to call the function recursively to create the left child, we simply pass in start and $\texttt{start} + \texttt{l_count}$ for the start and end. We pass in $\texttt{start} + \texttt{l_count}$ and end for the right child.
</p>

<br>

<h3 align="left">Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.</h3>

<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/part2/maxplanck.png" align="middle" width="400px"/>
        <figcaption>maxplanck.dae</figcaption>
      </td>
      <td>
        <img src="images/part2/CBlucy.png" align="middle" width="400px"/>
        <figcaption>CBlucy.dae</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/part2/beast.png" align="middle" width="400px"/>
        <figcaption>beast.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3 align="left">Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.</h3>

<div align="middle">
  <table style="width:100%;border:1px solid black;border-collapse:collapse;">
    <tr align="left">
      <td style="border:1px solid black;"></td>
      <th style="border:1px solid black;">teapot.dae</th>
      <th style="border:1px solid black;">cow.dae</th>
      <th style="border:1px solid black;">beetle.dae</th>
      <th style="border:1px solid black;">peter.dae</th>
      <th style="border:1px solid black;">beast.dae</th>
    </tr>
    <br>
    <tr align="left">
      <th style="border:1px solid black;">Number of Primitives</th>
      <td style="border:1px solid black;">2464</td>
      <td style="border:1px solid black;">5856</td>
      <td style="border:1px solid black;">7558</td>
      <td style="border:1px solid black;">40018</td>
      <td style="border:1px solid black;">64618</td>
    </tr>
    <br>
    <tr align="left">
      <th style="border:1px solid black;">Rendering w/o BVH</th>
      <td style="border:1px solid black;">9.1695</td>
      <td style="border:1px solid black;">23.8812</td>
      <td style="border:1px solid black;">31.2788</td>
      <td style="border:1px solid black;">N/A</td>
      <td style="border:1px solid black;">N/A</td>
    </tr>
    <br>
    <tr align="left">
      <th style="border:1px solid black;">Rendering w/o BVH</th>
      <td style="border:1px solid black;">9.1695</td>
      <td style="border:1px solid black;">23.8812</td>
      <td style="border:1px solid black;">31.2788</td>
      <td style="border:1px solid black;">N/A</td>
      <td style="border:1px solid black;">N/A</td>
    </tr>
    <br>
    <tr align="left">
      <th style="border:1px solid black;">Building BVH</th>
      <td style="border:1px solid black;">0.0050</td>
      <td style="border:1px solid black;">0.0215</td>
      <td style="border:1px solid black;">0.0182</td>
      <td style="border:1px solid black;">0.1348</td>
      <td style="border:1px solid black;">0.2451</td>
    </tr>
    <br>
    <tr align="left">
      <th style="border:1px solid black;">Rendering w/ BVH</th>
      <td style="border:1px solid black;">0.0631</td>
      <td style="border:1px solid black;">0.0678</td>
      <td style="border:1px solid black;">0.0662</td>
      <td style="border:1px solid black;">0.0628</td>
      <td style="border:1px solid black;">0.0585 (0.1014)</td>
    </tr>
  </table>
</div>
<br>

<p>
  We obtain these times using 480x360, 1 ray per pixel, 8 threads and at the default view point. We find that our BVH implementation reduces the rendering time by about 2 orders of magnitude when including the overhead of creating the BVH. We find that the rendering times are close to constant with respect to the number of primitives, but the time to construct the BVH scales linearly. For the large meshes, peter.dae and beast.dae, their reported render times are underestimated because their default views don’t take up the entire frame, causing most rays to miss entirely and the checks to terminate early. However, when focused on beast.dae’s back, the render time (reported in parentheses) is as expected.
</p>

<br>

<h2 align="middle">Part 3: Direct Illumination</h2>

<br>

<h3 align="left">Walk through both implementations of the direct lighting function.</h3>
  
<p>
  We wrote two sampling schemes to compute direct illumination. First, we did the naive method of sampling over the entire hemisphere with cosine weighting to account for Lambert’s law (hemisphere sampling). For each ray we sampled, we checked if its first intersection was with a light source. If so, we took the radiance of the light source from that point and multiplied by the cosine of the angle relative to the source due to Lambert’s law. We repeated this for the sample per light source rate (multiplied by the number of light sources).
</p>

<p>
  The other method we used was lighting samling, in which we took sample rays only in the directions of the light sources in the scene. For point light sources, we use a single sample (since only a single point is needed), and for area light sources, we use a number of samples per source. Using the conversion of the integral from class, we then can average over the samples from each source. For each ray, we compute if it intersects an object before reaching the light source, and if it does, we say that ray provides no direct lighting. If it does not intersect any object, we then compute the contribution to the lighting. Our estimator is now changed, since we have to account for sampling only the area of the light source. The radiance and PDF information is provided by the $\texttt{sample_L()}$ method inherent to the light source.
</p>

<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/part3/banana.png" align="middle" width="400px"/>
        <figcaption>Point lighting</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3 align="left">Show some images rendered with both implementations of the direct lighting function.</h3>

<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <th>
        Uniform Hemisphere Sampling
      </th>
      <th>
        Light Sampling
      </th>
    </tr>
    <tr align="center">
      <td>
        <img src="images/part3/bunny_1_64_h.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae 64 light rays</figcaption>
      </td>
      <td>
        <img src="images/part3/bunny_1_64_d.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae 64 light rays</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/part3/spheres_1_64_h.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
      <td>
        <img src="images/part3/spheres_1_64_d.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3 align="left">Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.</h3>

<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/part3/bunny_1_1_d.png" align="middle" width="400px"/>
        <figcaption>1 Light Ray (CBBunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/part3/bunny_1_4_d.png" align="middle" width="400px"/>
        <figcaption>4 Light Rays (CBBunny.dae)</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/part3/bunny_1_16_d.png" align="middle" width="400px"/>
        <figcaption>16 Light Rays (CBBunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/part3/bunny_1_64_d.png" align="middle" width="400px"/>
        <figcaption>64 Light Rays (CBBunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<p>
  In the soft shadows, most clearly below the bunny’s head for example, the smoothness increases dramatically with the number of samples. When there is 1 sample for the light, each pixel in the shadow is either illuminated or not, leading to a lot of noise in the shadow’s penumbra, as some pixels toward the complete shadow happen to hit the light, and some close to the edge happen not to hit the light. There is no smoothness since the pixel is only either lighted or not. Even with 4 samples, we start seeing some smoothness in the penumbra and less noise, yet there are still some pixels that jump out, particularly those at the edges of the penumbra that happen to have one or two unlucky samples that make the pixels look discontinuous with their neighbors. With 64 samples, the penumbra transitions quite smoothly from the region of total shadow and total light. In fact, it should be noted that with one sample per pixel, we even get noise in the regions of total light, since sampling different points in the light source will lead to different intensities, which will have noise relative to the average intensity that we aim to capture. This can be particularly noted high on the walls, where there is dramatically variable distance to the near edge and far edge of the light source, leading to noise in sampling from the light source. This noise emerges due to the fall off of illumination, even though each sample in these regions is indeed reaching the light source.
</p>

<br>

<h3 align="left">Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.</h3>

<p>
  The uniform hemisphere sampling is very noisy compared to the lighting sampling. This is because for a small light source, the majority of hemisphere sampled rays will miss the light source. To reduce noise, we need to sample a sufficient amount of rays such that a significant fraction are directed toward the light source. Therefore, we need a sampling rate that is comparable to the inverse of the solid angle of the light source from the point of interest. This is clearly very large for a small area light source and explains the significant noise observed even when using a large number of samples per light source under hemisphere sampling. The effect is particularly pronounced at points where the solid angle of the light source is diminished, such as points high on the walls. With lighting sampling, we completely avoid this issue by specifically sampling only rays that are aimed toward the light source. Thus, we reduce the number of samples needed by a factor of the solid angle of the light source. This is why even though using 64 uniform hemisphere samples produces an extremely noisy render, using 4 lighting samples can produce a relatively passable render. Using lighting sampling becomes increasingly important as the light source approaches a point, where hemisphere sampling will always fail (zero probability of hitting the light) and lighting sampling needs only 1 sample (for the point itself!).
</p>

<br>

<h2 align="middle">Part 4: Global Illumination</h2>

<br>

<h3 align="left">Walk through your implementation of the indirect lighting function.</h3>

<p>
  To compute the indirect lighting function, we start with a given ray. We compute the zero-bounce lighting by checking if the ray’s first intersection is a light source. Next, we call the recursive $\texttt{at_least_one_bounce_radiance()}$ function on the intersection point and return the sum of the zero (direct) and at least one bounce (indirect) light, as this gives the total radiance. The $\texttt{at_least_one_bounce_radiance()}$ function takes the intersection point and generates an incoming ray sample for the point. We then compute the direct light coming from that ray and recursively call the indirect light function on the intersection point with the new ray. To terminate the recursion, we have two end conditions. First, we have $\texttt{max_ray_depth}$, an argument that dictates the maximum number of bounces. If we reach this, we terminate and ignore any light with more bounces. Second, if the ray does not intersect any objects in the scene, we return zero for the ray. Thus, we are able to capture both direct and indirect light up to a specified depth.
</p>

<p>
  However, this will always be an underestimate, since we ignore light at further depths, and sometimes, more depth will be necessary if we have highly reflective surfaces (limited absorption). Our estimate of the lighting is therefore biased under the method described above. Accordingly, we also implement a Russian Roulette algorithm as a third stopping criterion for the recursion. Under this scheme, at any step, we stop with probability $p$. Then, we weight the computed radiance of a ray by $1/(1-p)$, such that we add the correct radiance in expectation. In the limit of having infinite maximum bounce depth (subject to random termination), we have an unbiased estimate of the true radiance with infinite bounces. In practice, by choosing $p$ appropriately, we will always terminate in a reasonable number of steps, as the probability of proceeding for a high number of steps decays exponentially.
</p>

<br>

<h3 align="left">Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.</h3>

<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/part4/bench_1024_4_accum_5.png" align="middle" width="400px"/>
        <figcaption>bench.dae</figcaption>
      </td>
      <td>
        <img src="images/part4/dragon_1024_4_accum_5.png" align="middle" width="400px"/>
        <figcaption>dragon.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3 align="left">Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)</h3>

<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/part4/bunny_1024_4_direct.png" align="middle" width="400px"/>
        <figcaption>Only direct illumination (CBBunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/part4/bunny_1024_4_indirect.png" align="middle" width="400px"/>
        <figcaption>Only indirect illumination (CBBunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<p>
  The direct illumination includes the zero bounce and one bounce light. The indirect illumination includes all illumination beyond two bounces. For the image shown here, we impose a maximum ray depth of 5 bounces. We see that the direct illumination provides the majority of the light on most surfaces (except the shadowed regions), but that this light is quite hard. The indirect light is softer and provides a more uniform glow around the environment. When we overlay the direct and indirect light, we get a more robust and rich visual experience, with the sharpness of the direct light and the softness of the indirect light. We also see that for the material parameters in this rendering, the indirect illumination provides a sizable amount of light as the images are not that different in their brightnesses.
</p>

<br>

<h3 align="left">For CBbunny.dae, mth bounce with max_ray_depth set to 0, 1, 2, 3, 4, 5 (the -m flag). Use 1024 samples per pixel.</h3>

<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/part4/bunny_1024_4_level_0.png" align="middle" width="400px"/>
        <figcaption>$\texttt{max_ray_depth} = 0$ (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/part4/bunny_1024_4_level_1.png" align="middle" width="400px"/>
        <figcaption>$\texttt{max_ray_depth} = 1$ (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center"><td>
        <img src="images/part4/bunny_1024_4_level_2.png" align="middle" width="400px"/>
        <figcaption>$\texttt{max_ray_depth} = 2$ (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/part4/bunny_1024_4_level_3.png" align="middle" width="400px"/>
        <figcaption>$\texttt{max_ray_depth} = 3$ (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/part4/bunny_1024_4_level_4.png" align="middle" width="400px"/>
        <figcaption>$\texttt{max_ray_depth} = 4$ (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/part4/bunny_1024_4_level_5.png" align="middle" width="400px"/>
        <figcaption>$\texttt{max_ray_depth} = 5$ (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
  
<p>
  These renders use 4 light rays with 1024 samples per pixel. For the second bounce of light, we are able to see that the shadowy side of the bunny starts to be indirectly illuminated by the light bouncing off the walls. This causes the right side of the bunny to be a pale red and the right to be pale blue. The underside is illuminated by light bouncing off the ground. The third bounce is more subtle, but we see that the shadow of the bunny gets lighter as the light that bounced from the underside of the bunny illuminates the shadow (first bounce anywhere, second bounce underside of bunny, third bounce floor below bunny).
</p>

<br>

<h3 align="left">For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 4 (the -m flag). Use 1024 samples per pixel.</h3>

<p>
  These renders use 4 light rays with 1024 samples per pixel.
</p>

<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/part4/bunny_1024_4_accum_0.png" align="middle" width="400px"/>
        <figcaption>$\texttt{max_ray_depth} = 0$ (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/part4/bunny_1024_4_accum_1.png" align="middle" width="400px"/>
        <figcaption>$\texttt{max_ray_depth} = 1$ (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/part4/bunny_1024_4_accum_2.png" align="middle" width="400px"/>
        <figcaption>$\texttt{max_ray_depth} = 2$ (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/part4/bunny_1024_4_accum_3.png" align="middle" width="400px"/>
        <figcaption>$\texttt{max_ray_depth} = 3$ (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/part4/bunny_1024_4_accum_4.png" align="middle" width="400px"/>
        <figcaption>$\texttt{max_ray_depth} = 4$ (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/part4/bunny_1024_4_accum_5.png" align="middle" width="400px"/>
        <figcaption>$\texttt{max_ray_depth} = 5$ (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3 align="left">For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Russian Roulette. Use 1024 samples per pixel.</h3>

<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/part4/bunny_1024_4_rr_0.png" align="middle" width="400px"/>
        <figcaption>$\texttt{max_ray_depth} = 0$ (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/part4/bunny_1024_4_rr_1.png" align="middle" width="400px"/>
        <figcaption>$\texttt{max_ray_depth} = 1$ (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/part4/bunny_1024_4_rr_2.png" align="middle" width="400px"/>
        <figcaption>$\texttt{max_ray_depth} = 2$ (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/part4/bunny_1024_4_rr_3.png" align="middle" width="400px"/>
        <figcaption>$\texttt{max_ray_depth} = 3$ (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/part4/bunny_1024_4_rr_4.png" align="middle" width="400px"/>
        <figcaption>$\texttt{max_ray_depth} = 4$ (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/part4/bunny_1024_4_rr_100.png" align="middle" width="400px"/>
        <figcaption>$\texttt{max_ray_depth} = 100$ (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
  
<p>
  The above renders were computed using a stopping probability of 0.2. As with the previous analysis of a fixed depth, we see that adding levels to the depth provides a richer illumination with greater indirect illumination. However, we find that when we use a fixed depth, there is an increase in brightness with each layer associated with the diffuse indirect light from each level of bounces. In Russian roulette on the other hand, we see a less dramatic increase in brightness. There is still some increase in the brightness, since the Russian roulette estimator is biased when we impose a maximum depth, but the effect is greatly diminished. This is because many rays exit before reaching the maximum depth and have their brightnesses adjusted accordingly. Overall, the renders are quite similar to the corresponding renders where all rays traverse the maximum depth, but these were computed in dramatically decreased time when compared to the maximum depth renders. We could get even further decreased time by increasing the stopping probability at the cost of introducing additional noise. For our given stopping probability and choice of 1024 samples per pixel, we do not see a severe amount of noise. We also see that we can reach maximum depths (100 bounces) that are unobtainable in a standard algorithm where all rays are traced to this distance.
</p>

<br>

<h3 align="left">Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.</h3>

<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/part4/spheres_1_4_accum_5.png" align="middle" width="400px"/>
        <figcaption>1 sample per pixel (CBspheres.dae)</figcaption>
      </td>
      <td>
        <img src="images/part4/spheres_2_4_accum_5.png" align="middle" width="400px"/>
        <figcaption>2 samples per pixel (CBspheres.dae)</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <<td>
        <img src="images/part4/spheres_4_4_accum_5.png" align="middle" width="400px"/>
        <figcaption>4 samples per pixel (CBspheres.dae)</figcaption>
      </td>
      <td>
        <img src="images/part4/spheres_8_4_accum_5.png" align="middle" width="400px"/>
        <figcaption>8 samples per pixel (CBspheres.dae)</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/part4/spheres_16_4_accum_5.png" align="middle" width="400px"/>
        <figcaption>16 samples per pixel (CBspheres.dae)</figcaption>
      </td>
      <td>
        <img src="images/part4/spheres_64_4_accum_5.png" align="middle" width="400px"/>
        <figcaption>64 samples per pixel (CBspheres.dae)</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/part4/spheres_1024_4_accum_5.png" align="middle" width="400px"/>
        <figcaption>1024 samples per pixel (CBspheres.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<p>
  Here we show rendered views of the Cornell box with Lambertian spheres at the different pixel sampling rates. Each render looks approximately the same in expectation, just with different levels of noise. The one sample per pixel render is very noisy relative to the 1024 sample per pixel render which has essentially no discernible noise. The 64 sample per pixel render has noise that appears like a grain on the image, and the 16 sample per pixel render appears slightly grainier. By 1 sample, it is hard to think of the noise as grain, as it is extremely noisy. However, all images do show soft shadows and indirect lighting associated with several bounces, as we use 5 levels maximum bounces in each of these renders. The effect of fewer samples per pixel is essentially that we increase grain, which we can associate in a real camera with a shorter exposure time or equivalently, a smaller aperture size (fewer photons reach the sensor).
</p>

<br>

<h2 align="middle">Part 5: Adaptive Sampling</h2>

<br>

<h3 align="left">Explain adaptive sampling. Walk through your implementation of the adaptive sampling.</h3>

<p>
  For adaptive sampling, we impose a maximum number of samples per pixel and a frequency at which to check for convergence. Each time we reach the frequency, we compute a 95% confidence interval for the illumination of the pixel. If the width of the confidence interval is less than a specified fraction of the estimated illumination, then we break, assuming we are sufficiently converged. If the width is wider, we continue to take more samples and check the next time we reach the frequency. We compute the 95% confidence interval by using the sample standard deviation and multiplying by 1.96 (assuming a normal distribution). We check for convergence only at the frequency to reduce the number of calculations for each pixel sample. The estimate we make for the illumination is the average over the number of samples that we take (potentially less than the maximum number of samples). We find that we are able to generate passable images with a tolerance of 5% noise in an order of magnitude less time than our runs without adaptive sampling.
</p>

<br>

<h3 align="left">Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.</h3>

<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/part5/bunny_2048_1_adapt.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBBunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/part5/bunny_2048_1_adapt_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBBunny.dae)</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/part5/bench_2048_1_adapt.png" align="middle" width="400px"/>
        <figcaption>Rendered image (bench.dae)</figcaption>
      </td>
      <td>
        <img src="images/part5/bench_2048_1_adapt_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (bench.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<p>
  We render these images with 2048 samples per pixel, 1 sample per light, Russian roulette, maximum depth of 100 bounces, and a sample batch frequency of 16 pixels with a tolerance of 0.05. We see that shadowy and darker regions require more samples per pixel, likely due to a greater portion of the illumination coming from indirect lighting. In addition, our convergence criterion is a relative condition, that the noise is a fraction of the illumination. Therefore, for dark regions, we impose a tighter absolute exit condition, meaning that more samples are needed. These two effects together lead to the observed sampling rates. The light source converges very quickly, since all rays have a very similar bright intensity and the light source (in this image) has a low reflectance. Interestingly, the dark region of the floor in the bench shows a high sampling rate needed, likely due to the reflectance being near zero, but slightly greater than zero. This means many samples are needed for convergence, even though it looks black on a glance.
</p>

<br>

<h2 align="middle">Part 6: Extra Credit</h2>

<br>

<h3 align="left">Optimizing the BVH: Implementing SAH and faster construction</h3>

<p>
  Our method for finding the best split has a few optimizations, one being the SAH, which speeds up the rendering. Our implementation is described in Part 2. We can easily modify our code to test splitting at the median of the widest axis, by setting the cost to be the difference between the number of left and right primitives. We also test splitting at the midpoint of the widest axis. The results are tabulated. We find that SAH significantly outperforms other methods. 
</p>

<p>
  The second optimization we make is during construction. We do not test every single possible split. For a bounding box, we can evaluate each split between every primitive, but we instead just evaluate a fixed number of possible splits. The reason is that we expect the latter to sufficiently approximate the optimal split of the exhaustive search. The tradeoff to this approximation is that we test a constant number of splits, rather than N splits, where N is the number of primitives.
</p>

<div align="middle">
  <table style="width:100%;border:1px solid black;border-collapse:collapse;">
    <tr align="left">
      <td style="border:1px solid black;"></td>
      <th style="border:1px solid black;">peter.dae</th>
      <th style="border:1px solid black;">maxplanck.dae</th>
      <th style="border:1px solid black;">beast.dae</th>
    </tr>
    <br>
    <tr align="left">
      <th style="border:1px solid black;">Primitives</th>
      <td style="border:1px solid black;">40018</td>
      <td style="border:1px solid black;">50801</td>
      <td style="border:1px solid black;">64618</td>
    </tr>
    <br>
    <tr align="left">
      <th style="border:1px solid black;">SAH</th>
      <td style="border:1px solid black;">0.0665</td>
      <td style="border:1px solid black;">0.0811</td>
      <td style="border:1px solid black;">0.0576</td>
    </tr>
    <br>
    <tr align="left">
      <th style="border:1px solid black;">Center</th>
      <td style="border:1px solid black;">0.2773</td>
      <td style="border:1px solid black;">4.4816</td>
      <td style="border:1px solid black;">7.9226</td>
    </tr>
    <br>
    <tr align="left">
      <th style="border:1px solid black;">Median</th>
      <td style="border:1px solid black;">0.1321</td>
      <td style="border:1px solid black;">0.3057</td>
      <td style="border:1px solid black;">0.1582</td>
    </tr>
  </table>
</div>
<br>

<h2 align="middle">Collaboration Statement</h2>

<p>
  Dawson wrote the majority of the initial code. Joshua worked through and debugged the code, as well as writing the code for several subtasks. Together, all code was found to match the example renders provided and appeared to produce visually satisfactory results. The writing of the report was shared, as both partners understood and could discuss the entire assignment. Most renders were carried out by Joshua, as his machine was found to render at a faster rate that Dawson’s. All renders took reasonable time. 
</p>
  
</body>
</html>
